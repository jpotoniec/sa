{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresja liniowa za pomocą TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przygotowanie danych\n",
    "\n",
    "Ponownie wykorzystamy problem przewidywania cen domów w Bostonie. Poniższy kod wczytuje dane i dzieli na trzy podzbiory: uczący, walidujący i testowy w proporcji 70%/10%/20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "X = boston['data']\n",
    "y = boston['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, p = X.shape\n",
    "n_train = int(.7*n)\n",
    "n_validation = int(.1*n)\n",
    "indices = np.random.permutation(n)\n",
    "X = X[indices]\n",
    "y = y[indices]\n",
    "X_train = X[:n_train, :]\n",
    "y_train = y[:n_train]\n",
    "X_validation = X[n_train:n_train+n_validation, :]\n",
    "y_validation = y[n_train:n_train+n_validation]\n",
    "X_test = X[n_train+n_validation:, :]\n",
    "y_test = y[n_train+n_validation:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konstrukcja grafu obliczeń\n",
    "\n",
    "TensorFlow oparty jest na koncepcji grafu obliczeń. Raz skonstruowany graf można wykorzystywać dla różnych danych wejściowych. Taka architektura umożliwa też automatyczne obliczanie gradientów i ich propagację w procesie optymalizacji. Konstrukcję grafu rozpoczniemy od usunięcia wszystkich operacji z domyślnego grafu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utworzymy placeholdery na dane wejściowe: macierz X i macierz y. Placeholdery można traktować jako argumenty funkcji: dopóki funkcja nie zostanie wywołana, to zmienne nie mają wartości - tak samo placeholdery nie mają wartości, dopóki nie uruchomimy obliczeń w grafie. Funkcja `tf.placeholder` wymaga dwóch argumentów: typu danych (tutaj: `tf.float32`, czyli 32-bitowy typ zmiennoprzecinkowy) oraz typu (shape) tensora (macierzy, wektora itd.), który zostanie podany. Opcjonalnie pierwszy wymiar może zostać zastąpiony przez `None` i wtedy można podawać tensory o różnych długościach w pierwszym wymiarze, natomiast pozostałe wymiary muszą być zdefiniowane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_pl = tf.placeholder(tf.float32, shape=(None, p)) # Macierz 2D, dowolna liczba wierszy, p kolumn\n",
    "y_pl = tf.placeholder(tf.float32, shape=(None,)) # Wektor o dowolnej liczbie wierszy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie utworzymy w grafie zmienne: zmienną `W` w formie wektora typu $p\\times 1$ oraz zmienną skalarną `b`. Obie te zmienne będą podlegały optymalizacji w procesie uczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.get_variable(\"W\", dtype=tf.float32, shape=(p,1))\n",
    "b = tf.get_variable(\"b\", dtype=tf.float32, shape=(1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodajemy do grafu dwie operacje: obliczania wartość predykcji (wektor `y_pred`) oraz obliczania wartość błędu średniokwadratowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = X_pl@W + b\n",
    "mse = tf.reduce_mean((y_pred-y_pl)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chcemy, żeby zmienne `W` oraz `b` automatycznie się zoptyamlizowały w procesie uczenia. Wykorzystamy w tym celu klasę `tf.train.AdamOptimizer`, która implementuje pewne ulepszenie algorytmu Gradient Descent. Dodajemy do grafu operator `minimizer`, wygenerowany przez instancję klasy `AdamOptimizer`. Celem tego operatora jest minimalizowane wartości `mse` podanej jako argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer()\n",
    "minimizer = opt.minimize(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utworzone w procesie konstrukcji grafu zmienne nie mają przypisanych wartości początkowych. Żeby to zrobić dodajemy do grafu kolejny operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uruchomienie obliczeń\n",
    "\n",
    "Wykonywanie obliczeń zdefiniowanych przez graf odbywa się za pomocą sesji. Rozpoczniemy od utworzenia obiektu sesji i wykonania operatora `init`, który przypisze wartości początkowe zmiennym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementujemy uczenie mini-batch przez `n_epoch` epok. \n",
    "\n",
    "1. Rozpoczynamy od wybrania `batch_size` indeksów spośród wszystkich `n_train` obiektów uczących. \n",
    "2. Przygotowujemy słownik `feed_dict`, w którym kluczami są placeholdery, a wartościami wartości, które mają zostać przypisane placeholderom. \n",
    "3. Każemy obliczyć wartości wyrażeń `minimizer` i `mse`, podając `feed_dict` jako słownik z wartościami placeholderów. TensorFlow wykonuje obliczenia inteligentnie w tym sensie, że do wykonania minimizer musi obliczyć ileś wyrażeń pośrednich, w tym wartość wyrażenia `mse`: zostaną wykonane wyłącznie niezbędne działania w grafie i każde dokładnie raz. Wartość wyrażenia `mse` zostanie obliczona również tylko raz, ale ponieważ jawnie podaliśmy ją do wykonania, jej wartość zostanie zwrócona jako wynik wykonania metody `run`.\n",
    "4. Zapamiętujemy wartość `mse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse_values = []\n",
    "batch_size = 100\n",
    "n_epoch = 1000\n",
    "for epoch in range(n_epoch):\n",
    "    indices = np.random.choice(n_train, size=batch_size)\n",
    "    feed_dict = {X_pl: X_train[indices,:], y_pl: y_train[indices]}\n",
    "    _, mse_value = sess.run([minimizer, mse], feed_dict)\n",
    "    mse_values.append(mse_value)\n",
    "plt.plot(mse_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uruchom ponownie powyższą komórkę z kodem. Zaobserwuj, że wykres wygląda  zupełnie inaczej: sesja jest ciągle aktywna, więc wartości zmiennych są pamiętane. Ponieważ zmienne `W` i `b` zostały już częściowo zoptymalizowane, wiec i wykres wygląda zupełnie inaczej. Żeby powrócić do stanu początkowego musisz ponownie wywołać operator `init`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zaimplementuj early stopping: Co 100-tną epokę uczenia oblicz wartość `mse` podając jako wartość `X_pl` macierz `X_validation`, a jako `y_pl` y_validation. Pamiętaj, żeby wtedy **nie** uruchamiać operatora `minimizer`, bo na zbiorze walidującym nie dokonujemy uczenia. Zapamiętuj najlepsze wartości `mse` i numery epok uczenia, w których je osiągnięto. Przerwij uczenie jeżeli przez ostatnie 1000 epok nie udało się znaleźć lepszej wartości `mse`. Zapamiętuj warotści `mse` uzyskane podczas uczenia i podczas walidacji (odpowiednio w `train_mses` i `validation_mses`), a następnie narysuj je na wykresie. Wypisz, w którym kroku zostało przerwane uczenie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(init)\n",
    "train_mses = []\n",
    "validation_mses = []\n",
    "for epoch in range(50000):\n",
    "    ...\n",
    "plt.plot(...)\n",
    "plt.plot(...)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularyzacja L1\n",
    "\n",
    "Zaimplementuj regularyzację L1:\n",
    "\n",
    "1. Utwórz placeholder na hiperparameter alpha\n",
    "2. Utwórz operator `cost` zgodnie z poniższym wzorem: \n",
    "$$ cost = MSE + \\alpha \\sum_{i=1}^n \\left|w_i\\right| $$\n",
    "Do zsumowania wszystkich wartości z macierzy `W` wykorzystaj funkcję `tf.reduce_sum`, a do obliczenia wartości bezwzględnej `tf.abs`.\n",
    "3. Utwórz nowy operator do optymalizacji wartości `cost` i nowy operator do inicjowania wartości zmiennych `init`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_pl = tf.placeholder(tf.float32)\n",
    "cost = mse + alpha_pl*tf.reduce_sum(tf.abs(W))\n",
    "cost_minimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przetestuj uzyskane rozwiązanie: ucz przez 10000 epok dla wartości każdej z następujacych wartości $\\alpha$: 0.01, 0.1, 1, 10, 100. Za każdym razem uruchamiaj operator `init`. Zbieraj co 100 epok MSE na zbiorze walidującym i narysuj je na wspólnym wykresie dla wszystkich 5 wartości hiperparametru $\\alpha$. Która z wartości $\\alpha$ jest najlepsza?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mses = {}\n",
    "for alpha in [0.01, 0.1, 1, 10, 100]:    \n",
    "    sess.run(init)\n",
    "    mses[alpha] = [] # tu zahcowuj wartości błędu na zbiorze walidującym\n",
    "    for epoch in range(10000):\n",
    "        ...\n",
    "        if epoch % 100 == 0:\n",
    "            ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for alpha in sorted(mses.keys()):\n",
    "    plt.plot(mses[alpha])\n",
    "plt.legend([str(alpha) for alpha in sorted(mses.keys())])\n",
    "plt.show()\n",
    "for alpha in sorted(mses.keys()):\n",
    "    plt.plot(mses[alpha][-10:])\n",
    "plt.legend([str(alpha) for alpha in sorted(mses.keys())])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Która z wartości $\\alpha$ jest najlepsza? ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
