{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rozpoznawanie ręcznie pisanych cyfr za pomocą TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pracę rozpoczniemy od pobrania zbioru MNIST, zawierającego 70 tyś. przykładów ręcznie pisanych cyfr 0-9 w formie obrazków 28x28 pikseli. Wykorzystamy w tym celu funkcję `fetch_mldata`, która pobera dane z serwisu [http://mldata.org/](mldata.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniższy kod dzieli losow dane na zbiory: uczący, walidujący i testowy w proporcji 70%/10%/20%. Wykorzystaj tak podzielone dane w dalszych krokach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n, p = mnist.data.shape\n",
    "k = 10 # liczba klas\n",
    "n_train = int(.7*n)\n",
    "n_validation = int(.1*n)\n",
    "indices = np.random.permutation(n)\n",
    "train_indices = indices[:n_train]\n",
    "validation_indices = indices[n_train:n_train+n_validation]\n",
    "test_indices = indices[n_train+n_validation:]\n",
    "X_train, y_train = mnist.data[train_indices,:], mnist.target[train_indices]\n",
    "X_validation, y_validation = mnist.data[validation_indices,:], mnist.target[validation_indices]\n",
    "X_test, y_test = mnist.data[test_indices,:], mnist.target[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja logitstyczna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stworzymy prosty model regresji logistycznej, uczony na surowych pikselach obrazków."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`W` to macierz $p\\times k$ (tzn. $p$ wag dla każdej z $k$ klas) zawierająca wagi cech (tutaj: pikseli obrazu), `b` to wyrazy wolne, różne dla każdej z `k` klas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.get_variable(dtype=tf.float32, name=\"W\", shape=(p,k))\n",
    "b = tf.get_variable(dtype=tf.float32, name=\"b\", shape=(k,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotwujemy placeholdery na cechy `X_pl` i poprawne etykiety `y_pl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_pl = tf.placeholder(dtype=tf.float32, shape=(None, p))\n",
    "y_pl = tf.placeholder(dtype=tf.int64, shape=(None,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczamy `logits`, tzn. wyjście regresji liniowej **przed** zastosowaniem funkcji softmax: każdy wiersz odpowiada jednemu przykładowi z `X_pl`, a każda kolumna jednej z klas. Gdyby każdy z wierszy unormować funkcją softmax, to w kolejnych wierszach byłyby prawdopodobieństwa, że obiekt należy do danej klasy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logits = X_pl@W+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczamy *cross entropy* dla każdego z przykładów (tzn. `ent` jest wektorem). Korzystamy z funkcji `sparse_softmax_cross_entropy_with_logits`, która jako argument `labels` przyjmuje \"normalne\" etykiety klas, a jako `logits` nieunormowane prawdopodobieństwa (w postaci jak opisana wyżej). Na bazie `ent` obliczamy średnią i otrzymujemy stratę `loss`, którą optymailzujemy za pomocą `AdamOptimizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_pl, logits=logits)\n",
    "loss = tf.reduce_mean(ent)\n",
    "minimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczamy wektor `y_pred` przez wybranie kolumny zawierającej największą wartość w każdym wierszu (*softmax* jest funkcją monotoniczną, więc do zrobienia predykcji nie potrzebujemy prawdopodobieństw - wystarczy wybrać największą wartość). Następnie obliczamy *accuracy*: \n",
    "1. tworzymy wektor `correct`, który zawiera `True` w tych komórkach, gdzie odpowiadające komórki `y_pred` i `y_pl` są równe oraz `False` w przeciwnym razie;\n",
    "2. rzutujemy wektor `correct` na liczby zmiennoprzecinkowe (`True` zamienia się w `1.0`, a `False`, w `0.0`);\n",
    "3. uśredniamy, żeby obliczyć *accuracy*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = tf.argmax(logits, axis=1)\n",
    "correct = tf.equal(y_pred, y_pl)\n",
    "correct = tf.cast(correct, dtype=tf.float32)\n",
    "acc = tf.reduce_mean(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Dalej jest już tak samo jak przy regresji liniowej: tworzymy operator do inicjalizacji zmiennych i obiekt sesji, inicjalizujemy zmienne i przez `n_epoch` trenujemy na batchach rozmiaru `batch_size`, zbierając wartości *loss* i *accuracy* na zbiorze uczącym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(init)\n",
    "loss_values = []\n",
    "acc_values = []\n",
    "batch_size = 100\n",
    "n_epoch = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epoch):\n",
    "    indices = np.random.choice(n_train, size=batch_size) \n",
    "    feed_dict = {X_pl: X_train[indices,:], y_pl: y_train[indices]}\n",
    "    _, loss_value = sess.run([minimizer, loss], feed_dict)\n",
    "    loss_values.append(loss_value)\n",
    "    if epoch % 100 == 0:\n",
    "        acc_val = sess.run(acc, {X_pl: X_validation, y_pl: y_validation})    \n",
    "        acc_values.append(acc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"Loss na zbiorze uczącym\")\n",
    "plt.plot(loss_values)\n",
    "plt.show()\n",
    "plt.title(\"Accuracy na zbiorze walidującym (co 100 epok)\")\n",
    "plt.plot(acc_values)\n",
    "plt.show()\n",
    "print(acc_values[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "Zadanie do samodzielnego wykonania: wykorzystaj powyższy kod i zaimplementuj *early stopping* zamiast trenowania przez stałą liczbę epok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu kod, może być w więcej niż jednej komórce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sieć neuronowa z jedną warstwą ukrytą\n",
    "\n",
    "Regresja logistyczna to wariant sieci neuronowej bez warstw ukrytych. Wykorzystaj powyższy kod i rozszerz go o warstwę ukrytą o h=500 neuronach.\n",
    "\n",
    "Potrzebujesz następujących zmiennych (wszystkie w typie zmiennoprzecinkowym):\n",
    "* `W1` macierz rozmiaru $p\\times h$\n",
    "* `b1` wektor rozmiaru $h$\n",
    "* `W2` macierz rozmiaru $h\\times k$\n",
    "* `b2` wektor rozmiaru $k$\n",
    "\n",
    "Logits mają zostać obliczone zgodnie z poniższym wzorem:\n",
    "$$ logits = elu(XW_1+b_1)W_2+b_2 $$\n",
    "gdzie funkcja $elu$ jest funkcją wprowadzającą nieliniowość zaimplementowaną w TensorFlow jako `tf.nn.elu`:\n",
    "$$ elu(x) = \\begin{cases} e^x - 1 & x<0 \\\\ x & x\\geq 0 \\end{cases} $$\n",
    "\n",
    "Zaimplementuj early stopping. W trakcie uczenia zbieraj wartości straty i accuracy na zbiorze uczącym i na zbiorze walidującym, a następnie narysuj je na wykresach. Jeżeli uczenie trwa zbyt wolno jak na Twój gust, zmniejsz `batch_size` i/lub liczbę neuronów w warstwie ukrytej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = 500\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tu kod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
